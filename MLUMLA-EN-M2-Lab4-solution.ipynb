{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13c926d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<center><img src=\"images/logo.png\" alt=\"drawing\" width=\"400\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "\n",
    "# ML through Application\n",
    "## Module 2, Lab 4: Using Logistic Regression\n",
    "\n",
    "In this notebook, you will build a [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model to predict the Outcome Type feature of the dataset. You will also look at how probability threshold calibration can help to improve a classifier's performance.\n",
    "Probability threshold calibration is a way to calibrate the model working on the output from the logistic regression model for binary classification. For example, if the model needs to predict class 1, you can put a threshold to consider that the the output belongs to class 1 only if the probability is greater than 80%. Otherwise, it belongs to class 0.\n",
    "\n",
    "You will learn how to do the following:\n",
    "\n",
    "- Do basic data processing.\n",
    "- Explain how and why you create training, testing, and validation datasets.\n",
    "- Use a pipeline and ColumnTransformers to process data.\n",
    "- Train a classifier.\n",
    "- Test the performance of a classifier.\n",
    "- Identify ways to improve a classifier by using probability threshold calibration.\n",
    "\n",
    "----\n",
    "\n",
    "__Austin Animal Center Dataset__\n",
    "\n",
    "In this lab, you will work with historical pet adoption data in the [Austin Animal Center Shelter Intakes and Outcomes dataset](https://www.kaggle.com/datasets/aaronschlegel/austin-animal-center-shelter-intakes-and-outcomes?resource=download). The target field of the dataset (**Outcome Type**) is the outcome of adoption: 1 for adopted and 0 for not adopted. Multiple features are used in the dataset.\n",
    "\n",
    "Dataset schema:\n",
    "- __Pet ID:__ Unique ID of the pet\n",
    "- __Outcome Type:__ State of pet at the time of recording the outcome (0 = not placed, 1 = placed). This is the field to predict.\n",
    "- __Sex upon Outcome:__ Sex of pet at outcome\n",
    "- __Name:__ Name of pet \n",
    "- __Found Location:__ Found location of pet before it entered the shelter\n",
    "- __Intake Type:__ Circumstances that brought the pet to the shelter\n",
    "- __Intake Condition:__ Health condition of the pet when it entered the shelter\n",
    "- __Pet Type:__ Type of pet\n",
    "- __Sex upon Intake:__ Sex of pet when it entered the shelter\n",
    "- __Breed:__ Breed of pet \n",
    "- __Color:__ Color of pet \n",
    "- __Age upon Intake Days:__ Age (days) of pet when it entered the shelter\n",
    "- __Age upon Outcome Days:__ Age (days) of pet at outcome\n",
    "\n",
    "----\n",
    "\n",
    "You will be presented with two kinds of exercises throughout the notebook: activities and challenges. <br/>\n",
    "\n",
    "| <img style=\"float: center;\" src=\"images/activity.png\" alt=\"Activity\" width=\"125\"/>| <img style=\"float: center;\" src=\"images/challenge.png\" alt=\"Challenge\" width=\"125\"/>|\n",
    "| --- | --- |\n",
    "|<p style=\"text-align:center;\">No coding is needed for an activity. You try to understand a concept, <br/>answer questions, or run a code cell.</p> |<p style=\"text-align:center;\">Challenges are where you can practice your coding skills.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6642d78",
   "metadata": {},
   "source": [
    "## Index\n",
    "\n",
    "- [Data processing](#Data-processing)\n",
    "- [Train a classifier](#Train-a-classifier)\n",
    "- [Test the classifier](#Test-the-classifier)\n",
    "- [Improvement ideas: Probability threshold calibration](#Improvement-ideas:-Probability-threshold-calibration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a61401",
   "metadata": {},
   "source": [
    "---\n",
    "## Data processing\n",
    "\n",
    "To get started, you will process the dataset the same way that you did in the previous labs. Run all the cells in this section before you move to the section where you train the model.\n",
    "\n",
    "One of the most important steps is to clean the data. You should do this _before_ you split the data into training, testing, and validation datasets to be more efficient with your coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add0af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# installing libraries\n",
    "!pip install -U -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa092e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d4d013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/review_dataset.csv\")\n",
    "\n",
    "print(\"The shape of the dataset is:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111975f2",
   "metadata": {},
   "source": [
    "To remember what the data looks like, print the names of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff0269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739e6d6d",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h3><i>Try it yourself!</i></h3>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"images/challenge.png\" alt=\"Challenge\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">In the following code cell, create four lists that contain the names of the numerical, categorical, text, and all features, in addition to a variable for the target (label).</p><br>\n",
    "    <p style=\" text-align: center; margin: auto;\">Use the following names for the lists: <code>numerical_features</code>, <code>categorical_features</code>, <code>text_features</code>, <code>model_features</code>, and <code>model_target</code>.</p>\n",
    "    <br>\n",
    "    <p style=\" text-align: center; margin: auto;\"> Note: Omit the Pet ID and Name features from the lists because they aren't relevant to the outcome.</p><br>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5834cc36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############### CODE HERE ###############\n",
    "\n",
    "numerical_features = [\"Age upon Intake Days\", \"Age upon Outcome Days\"]\n",
    "\n",
    "categorical_features = [\"Sex upon Outcome\", \"Intake Type\",\n",
    "                        \"Intake Condition\", \"Pet Type\", \"Sex upon Intake\"]\n",
    "\n",
    "text_features = [\"Found Location\", \"Breed\", \"Color\"]\n",
    "\n",
    "model_features = numerical_features + categorical_features + text_features\n",
    "\n",
    "model_target = \"Outcome Type\"\n",
    "\n",
    "############## END OF CODE ##############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b68586",
   "metadata": {},
   "source": [
    "### Clean numerical features\n",
    "\n",
    "As you did in the previous lab, examine the numerical features. Remember that the `value_counts()` function can provide a view of the numerical features by placing feature values in respective bins. The function can also be used to plot a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51f62ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in numerical_features:\n",
    "    print(df[c].value_counts(bins=10, sort=False))\n",
    "    df[c].value_counts(bins=10, sort=False).plot(kind=\"bar\", alpha=0.75, rot=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e2b360",
   "metadata": {},
   "source": [
    "If any outliers are identified as likely wrong values, dropping them could improve the numerical values histograms and overall model performance.\n",
    "\n",
    "In this lab, you will remove any values in the upper 10 percent for the feature and then plot the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ba0d2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in numerical_features:\n",
    "    # Drop values beyond 90% of max()\n",
    "    dropIndexes = df[df[c] > df[c].max() * 9 / 10].index\n",
    "    df.drop(dropIndexes, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd40d786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in numerical_features:\n",
    "    print(df[c].value_counts(bins=10, sort=False))\n",
    "    df[c].value_counts(bins=10, sort=False).plot(kind=\"bar\", alpha=0.75, rot=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd2a1d6",
   "metadata": {},
   "source": [
    "### Clean text features\n",
    "\n",
    "As you did in the previous lab, examine the text features. You can re-use the helper function from the previous notebook to process the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9164b19",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h3><i>Try it yourself!</i></h3>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"images/challenge.png\" alt=\"Challenge\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">In the following code cell, write the <code>preProcessText()</code>, <code>lexiconProcess()</code>, and <code>cleanSentence()</code> helper functions.</p><br>\n",
    "    <p style=\" text-align: center; margin: auto;\">You might need to refer to the notebook from the previous lab.</p>\n",
    "    <br>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459576ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare cleaning functions\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stop_words = [\"a\", \"an\", \"the\", \"this\", \"that\", \"is\", \"it\", \"to\", \"and\", \"in\"]\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "############### CODE HERE ###############\n",
    "\n",
    "def preProcessText(text):\n",
    "    # Lowercase text, and strip leading and trailing white space\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.compile('<.*?>').sub('', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n",
    "\n",
    "    # Remove extra white space\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"[0-9]\", \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def lexiconProcess(text, stop_words, stemmer):\n",
    "    filtered_sentence = []\n",
    "    words = text.split(\" \")\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(stemmer.stem(w))\n",
    "    text = \" \".join(filtered_sentence)\n",
    "\n",
    "    return text\n",
    "\n",
    "def cleanSentence(text, stop_words, stemmer):\n",
    "    return lexiconProcess(preProcessText(text), stop_words, stemmer)\n",
    "\n",
    "############## END OF CODE ##############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4328a4b",
   "metadata": {},
   "source": [
    "__Warning:__ The text cleaning process can take a long time to complete, depending on the size of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af6a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean the text features\n",
    "for c in text_features:\n",
    "    print(\"Text cleaning: \", c)\n",
    "    df[c] = [cleanSentence(item, stop_words, stemmer) for item in df[c].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d66f8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### Create training, test, and validation datasets\n",
    "\n",
    "Now that the data has been cleaned, you need to split the full dataset into training and test subsets by using sklearn's [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function. This function allows you to specify the following:\n",
    "\n",
    "- The proportion of the dataset to include in the test split, as a number in the range 0.0–1.0 with a default of 0.25.\n",
    "- An integer that controls the shuffling that is applied to the data before the split. Passing an integer allows for reproducible output across multiple function calls.\n",
    "\n",
    "To help reduce sampling bias, the original dataset is shuffled before the split. After the initial split, the training data is further split into training and validation subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc09c9e",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h3><i>Try it yourself!</i></h3>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"images/challenge.png\" alt=\"Challenge\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">In the following code cell, split the original dataset into training and test subsets by using sklearn's <code>train_test_split()</code> function.</p><br>\n",
    "    <p style=\" text-align: center; margin: auto;\">From the original DataFrame, create DataFrames that are named <code>train_data</code> and <code>test_data</code>.</p>\n",
    "    <p style=\" text-align: center; margin: auto;\">From the <code>train_data</code> DataFrame, create DataFrames that are named <code>train_data</code> and <code>val_data</code>.</p>\n",
    "    <br>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a9eef6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "############### CODE HERE ###############\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.15, shuffle=True, random_state=23)\n",
    "\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, shuffle=True, random_state=23)\n",
    "\n",
    "############## END OF CODE ##############\n",
    "\n",
    "# Print the shapes of the training, validation, and test datasets\n",
    "print(\n",
    "    \"Train - Test - Validation dataset shapes: \",\n",
    "    train_data.shape,\n",
    "    val_data.shape,\n",
    "    test_data.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e28e919",
   "metadata": {},
   "source": [
    "---\n",
    "### Process the data with a pipeline and ColumnTransformer\n",
    "\n",
    "In a typical ML workflow, you need to apply data transformations, such as imputation and scaling, at least twice: first on the training dataset by using `.fit()` and `.transform()` when preparing the data to train the model, and then by using `.transform()` on any new data that you want to predict on (validation or test). Sklearn's [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) is a tool that simplifies this process by enforcing the implementation and order of data processing steps, being important for reproducibility. In other words, all the data is transformed the same way each time that you process any part of it.\n",
    "\n",
    "In this section, you will build separate pipelines to handle the numerical, categorical, and text features. Then, you will combine them into a composite pipeline along with an estimator. To do this, you will use a [LogisticRegression classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "You will need multiple pipelines to ensure that all the data is handled correctly:\n",
    "\n",
    "* __Numerical features pipeline:__ Impute missing values with the mean by using sklearn's SimpleImputer, followed by a MinMaxScaler. If different processing is desired for different numerical features, different pipelines should be built as described for the text features pipeline. See the `numerical_processor` in the following code cell.\n",
    "\n",
    "* __Categoricals pipeline:__ Impute with a placeholder value (this won't have an effect because you already encoded the 'nan' values), and encode with sklearn's OneHotEncoder. If computing memory is an issue, it is a good idea to check the number of unique values for the categoricals to get an estimate of how many dummy features one-hot encoding will create. Note the `handle_unknown` parameter, which tells the encoder to ignore (rather than throw an error for) any unique value that might show in the validation or test set that was not present in the initial training set. See the `categorical_processor` in the following code cell.\n",
    "\n",
    "* __Text features pipeline:__ With memory usage in mind, build three more pipelines, one for each of the text features. Current sklearn implementation requires a separate transformer for each text feature (unlike the numericals and categoricals).\n",
    "\n",
    "Finally, the selective preparations of the dataset features are then put together into a collective ColumnTransformer, which is used in a pipeline along with an estimator. This ensures that the transforms are performed automatically in all situations. This includes on the raw data when fitting the model, when making predictions, when evaluating the model on a validation dataset through cross-validation, or when making predictions on a test dataset in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632c079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "### COLUMN_TRANSFORMER ###\n",
    "##########################\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline(\n",
    "    [\n",
    "        (\"num_imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\n",
    "            \"num_scaler\",\n",
    "            MinMaxScaler(),\n",
    "        ),  # Shown in case it is needed. Not a must with decision trees.\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocess the categorical features\n",
    "categorical_processor = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"cat_imputer\",\n",
    "            SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "        ),  # Shown in case it is needed. No effect because you already imputed with 'nan' strings.\n",
    "        (\n",
    "            \"cat_encoder\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "        ),  # handle_unknown tells it to ignore (rather than throw an error for) any value that was not present in the initial training set.\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocess first text feature\n",
    "text_processor_0 = Pipeline(\n",
    "    [(\"text_vectorizer_0\", CountVectorizer(binary=True, max_features=50))]\n",
    ")\n",
    "\n",
    "# Preprocess second text feature\n",
    "text_processor_1 = Pipeline(\n",
    "    [(\"text_vectorizer_1\", CountVectorizer(binary=True, max_features=50))]\n",
    ")\n",
    "\n",
    "# Preprocess third text feature\n",
    "text_processor_2 = Pipeline(\n",
    "    [(\"text_vectorizer_2\", CountVectorizer(binary=True, max_features=50))]\n",
    ")\n",
    "\n",
    "# Combine all data preprocessors (add more if you choose to define more)\n",
    "# For each processor/step, specify: a name, the actual process, and the features to be processed.\n",
    "data_processor = ColumnTransformer(\n",
    "    [\n",
    "        (\"numerical_processing\", numerical_processor, numerical_features),\n",
    "        (\"categorical_processing\", categorical_processor, categorical_features),\n",
    "        (\"text_processing_0\", text_processor_0, text_features[0]),\n",
    "        (\"text_processing_1\", text_processor_1, text_features[1]),\n",
    "        (\"text_processing_2\", text_processor_2, text_features[2]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Visualize the data processing pipeline\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "data_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a76e70",
   "metadata": {},
   "source": [
    "---\n",
    "## Train a classifier\n",
    "\n",
    "To train a logistic regression model, you will use sklearn's [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fd85a6",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h3><i>It's time to check your knowledge!</i></h3>\n",
    "    <br>\n",
    "    <p style=\" text-align: center; margin: auto;\">To load the question, run the following cell.</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a880f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell for a knowledge check question\n",
    "from MLUMLA_EN_M2_Lab4_quiz_questions import *\n",
    "\n",
    "question_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7286bd",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "\n",
    "Update the pipeline that includes the desired data transformers to _include_ a logistic regression estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a45d224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### PIPELINE ###\n",
    "################\n",
    "\n",
    "# Pipeline with all desired data transformers, along with an estimator\n",
    "# Later, you can set/reach the parameters by using the names issued - for hyperparameter tuning, for example\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"data_processing\", data_processor),\n",
    "        (\n",
    "            \"lg\",\n",
    "            LogisticRegression(\n",
    "                solver=\"liblinear\", penalty=\"l1\", C=0.001, class_weight={0: 1, 1: 20}\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Visualize the pipeline\n",
    "# This will be helpful especially when building more complex pipelines, stringing together multiple preprocessing steps\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42090e7c",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "Finally, train the classifier with `.fit()` on the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f41ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get training data to train the classifier\n",
    "X_train = train_data[model_features]\n",
    "y_train = train_data[model_target]\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "# Training data going through the pipeline is imputed (with means from the training data),\n",
    "#   scaled (with the min/max from the training data),\n",
    "#   and finally used to fit the model\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a77c6a",
   "metadata": {},
   "source": [
    "---\n",
    "## Test the classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcd2483",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h3><i>Try it yourself!</i></h3>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"images/challenge.png\" alt=\"Challenge\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">Now that you have a trained classifier, you can evaluate the performance on the test dataset.</p>\n",
    "    <p style=\" text-align: center; margin: auto;\">In the following code cell, use <code>.predict()</code> on the <code>pipeline</code> to do this.</p>\n",
    "    <br>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8874be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Get validation data to validate the classifier\n",
    "X_test = test_data[model_features]\n",
    "y_test = test_data[model_target]\n",
    "\n",
    "# Use the fitted model to make predictions on the test dataset\n",
    "# Testing data going through the pipeline is imputed (with means from the training data),\n",
    "#   scaled (with the min/max from the training data),\n",
    "#   and finally used to make predictions\n",
    "############### CODE HERE ###############\n",
    "\n",
    "test_predictions = pipeline.predict(X_test)\n",
    "\n",
    "############## END OF CODE ##############\n",
    "\n",
    "print(\"Model performance on the test set:\")\n",
    "print(confusion_matrix(y_test, test_predictions))\n",
    "print(classification_report(y_test, test_predictions))\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c786a9",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h3><i>It's time to check your knowledge!</i></h3>\n",
    "    <br>\n",
    "    <p style=\" text-align: center; margin: auto;\">To load the question, run the following cell.</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433fec9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell for a knowledge check question\n",
    "question_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1203f9d7",
   "metadata": {},
   "source": [
    "---\n",
    "## Improvement ideas: Probability threshold calibration\n",
    "\n",
    "Other than tuning LogisticRegression hyperparameter values, another path to improve a classifier's performance is to dig deeper into how the classifier actually assigns class membership.\n",
    "\n",
    "### Comparing binary predictions and probability predictions\n",
    "\n",
    "You can use `classifier.predict()` to examine classifier binary predictions, although the outputs of most classifiers are real-valued, not binary. For most classifiers in sklearn, the `classifier.predict_proba()` method returns class probabilities as a 2D NumPy array of shape (n_samples, n_classes) where the classes are lexicographically ordered.\n",
    "\n",
    "For this example, you will look at the first five predictions made, in binary format and in real-valued probability format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8357fc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline.predict(X_test)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc40561a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline.predict_proba(X_test)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486bb8e9",
   "metadata": {},
   "source": [
    "How are the predicted probabilities used to decide class membership? On each row of `predict_proba` output, the probability values sum to 1. The array has two columns, one for each response class: column 0 is the predicted probability that each observation is a member of class 0, and column 1 is the predicted probability that each observation is a member of class 1. From the predicted probabilities, choose the class with the highest probability.\n",
    "\n",
    "The key here is that a threshold of 0.5 is used by default (for binary problems) to convert predicted probabilities into class predictions: class 0, if predicted probability is less than 0.5, and class 1, if predicted probability is greater than 0.5.\n",
    "\n",
    "Can you improve classifier performance by changing the classification threshold? Adjust the classification threshold to influence the performance of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afb4a4d",
   "metadata": {},
   "source": [
    "### Threshold calibration to improve model accuracy\n",
    "\n",
    "Calculate the accuracy by using different values for the classification threshold, and select the threshold that results in the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2b0528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the accuracy by using different values for the classification threshold,\n",
    "# and select the threshold that results in the highest accuracy.\n",
    "highest_accuracy = 0\n",
    "threshold_highest_accuracy = 0\n",
    "\n",
    "thresholds = np.arange(0, 1, 0.01)\n",
    "scores = []\n",
    "for t in thresholds:\n",
    "    # Set threshold to 't' instead of 0.5\n",
    "    y_test_other = (pipeline.predict_proba(X_test)[:, 1] >= t).astype(float)\n",
    "    score = accuracy_score(y_test, y_test_other)\n",
    "    scores.append(score)\n",
    "    if score > highest_accuracy:\n",
    "        highest_accuracy = score\n",
    "        threshold_highest_accuracy = t\n",
    "print(\n",
    "    \"Highest accuracy on test:\",\n",
    "    highest_accuracy,\n",
    "    \", Threshold for the highest accuracy:\",\n",
    "    threshold_highest_accuracy,\n",
    ")\n",
    "\n",
    "# Plot the accuracy against the threshold choices\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "plt.plot(\n",
    "    [0.5, 0.5],\n",
    "    [np.min(scores), np.max(scores)],\n",
    "    linestyle=\"--\",\n",
    "    color=\"blue\",\n",
    "    label=\"Default threshold (0.5)\",\n",
    ")\n",
    "plt.plot(\n",
    "    [threshold_highest_accuracy, threshold_highest_accuracy],\n",
    "    [np.min(scores), np.max(scores)],\n",
    "    linestyle=\"--\",\n",
    "    color=\"green\",\n",
    "    label=\"Threshold for highest accuracy ({})\".format(threshold_highest_accuracy),\n",
    ")\n",
    "plt.plot(thresholds, scores, marker=\".\", color=\"orange\")\n",
    "plt.title(\"Accuracy Compared to Threshold Choices\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), fancybox=True, shadow=True, ncol=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04ce183",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "## Conclusion\n",
    "\n",
    "This notebook showed you how to create pipelines to process your data and generate your first logistic regression model.\n",
    "\n",
    "## Next lab\n",
    "\n",
    "In the next lab, you will gain experience with two main types of hyperparameter tuning: grid search and randomized search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
