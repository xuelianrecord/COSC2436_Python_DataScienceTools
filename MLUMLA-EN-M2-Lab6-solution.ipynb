{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "072c4ad6",
   "metadata": {},
   "source": [
    "<center><img src=\"images/logo.png\" alt=\"drawing\" width=\"400\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "\n",
    "# ML through Application\n",
    "## Module 2, Lab 6: Using Ensemble Learning\n",
    "\n",
    "Ensemble methods create a strong model by combining the predictions of multiple weak models (also known as weak learners or base estimators) that are built with a given dataset and a given learning algorithm.\n",
    "\n",
    "You will learn how to do the following:\n",
    "\n",
    "- Explain what bagging is and how to use it.\n",
    "- Explain what a random forest is and how to use it.\n",
    "- Explain what boosting is and how to use it.\n",
    "\n",
    "----\n",
    "\n",
    "__Austin Animal Center Dataset__\n",
    "\n",
    "In this lab, you will work with historical pet adoption data in the [Austin Animal Center Shelter Intakes and Outcomes dataset](https://www.kaggle.com/datasets/aaronschlegel/austin-animal-center-shelter-intakes-and-outcomes?resource=download). The target field of the dataset (**Outcome Type**) is the outcome of adoption: 1 for adopted and 0 for not adopted. Multiple features are used in the dataset.\n",
    "\n",
    "Dataset schema:\n",
    "- __Pet ID:__ Unique ID of the pet\n",
    "- __Outcome Type:__ State of pet at the time of recording the outcome (0 = not placed, 1 = placed). This is the field to predict.\n",
    "- __Sex upon Outcome:__ Sex of pet at outcome\n",
    "- __Name:__ Name of pet \n",
    "- __Found Location:__ Found location of pet before it entered the shelter\n",
    "- __Intake Type:__ Circumstances that brought the pet to the shelter\n",
    "- __Intake Condition:__ Health condition of the pet when it entered the shelter\n",
    "- __Pet Type:__ Type of pet\n",
    "- __Sex upon Intake:__ Sex of pet when it entered the shelter\n",
    "- __Breed:__ Breed of pet \n",
    "- __Color:__ Color of pet \n",
    "- __Age upon Intake Days:__ Age (days) of pet when it entered the shelter\n",
    "- __Age upon Outcome Days:__ Age (days) of pet at outcome\n",
    "\n",
    "----\n",
    "\n",
    "You will be presented with a challenge in this notebook.<br/>\n",
    "\n",
    "| <img style=\"float: center;\" src=\"./images/challenge.png\" alt=\"Challenge\" width=\"125\"/>|\n",
    "| --- |\n",
    "|<p style=\"text-align:center;\">Challenges are where you can practice your coding skills.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d633e36",
   "metadata": {},
   "source": [
    "## Index\n",
    "\n",
    "- [Data preparation](#Data-preparation)\n",
    "- [Bagging](#Bagging)\n",
    "- [Random forest](#Random-forest)\n",
    "- [Boosting](#Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e7ae45",
   "metadata": {},
   "source": [
    "---\n",
    "## Data preparation\n",
    "\n",
    "Before you can build the first model, you need to import and prepare the data. You can follow the same steps as in the previous labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbb0e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install libraries\n",
    "!pip install -U -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d4bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re, string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    BaggingClassifier,\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0d6af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/review_dataset.csv\")\n",
    "\n",
    "print(\"The shape of the dataset is:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd441b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create lists of the features and name the target\n",
    "\n",
    "# Numerical features \n",
    "numerical_features = [\"Age upon Intake Days\", \"Age upon Outcome Days\"]\n",
    "\n",
    "# Drop the ID features: RescuerID and PetID\n",
    "categorical_features = [\n",
    "    \"Sex upon Outcome\",\n",
    "    \"Intake Type\",\n",
    "    \"Intake Condition\",\n",
    "    \"Pet Type\",\n",
    "    \"Sex upon Intake\",\n",
    "    \"Breed\",\n",
    "    \"Color\",\n",
    "]\n",
    "\n",
    "# Based on exploratory data analysis (EDA), select the text features\n",
    "text_features = [\"Found Location\"]\n",
    "\n",
    "model_features = numerical_features + categorical_features + text_features\n",
    "model_target = \"Outcome Type\"\n",
    "\n",
    "df[categorical_features + text_features] = df[\n",
    "    categorical_features + text_features\n",
    "].astype(\"str\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dc4bba",
   "metadata": {},
   "source": [
    "### Cleaning the text fields\n",
    "\n",
    "__Note:__ The cleaning stage can take a few minutes, depending on how much text needs to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527c4b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare cleaning functions\n",
    "\n",
    "stop_words = [\"a\", \"an\", \"the\", \"this\", \"that\", \"is\", \"it\", \"to\", \"and\"]\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "def preProcessText(text):\n",
    "    # Lowercase text, and strip leading and trailing white space\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.compile(\"<.*?>\").sub(\"\", text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.compile(\"[%s]\" % re.escape(string.punctuation)).sub(\" \", text)\n",
    "\n",
    "    # Remove extra white space\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def lexiconProcess(text, stop_words, stemmer):\n",
    "    filtered_sentence = []\n",
    "    words = text.split(\" \")\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(stemmer.stem(w))\n",
    "    text = \" \".join(filtered_sentence)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def cleanSentence(text, stop_words, stemmer):\n",
    "    return lexiconProcess(preProcessText(text), stop_words, stemmer)\n",
    "\n",
    "\n",
    "# Clean the text features\n",
    "for c in text_features:\n",
    "    print(\"Text cleaning: \", c)\n",
    "    df[c] = [cleanSentence(item, stop_words, stemmer) for item in df[c].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37906585",
   "metadata": {},
   "source": [
    "### Create training and test datasets\n",
    "\n",
    "As part of data preparation, the dataset is split into training and test subsets by using sklearn's [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function.\n",
    "\n",
    "For this notebook, you will use 90 percent of the data for the training set and 10 percent for the test set. Determine the best split based on the size of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc7fced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(\n",
    "    df, test_size=0.1, shuffle=True, random_state=23\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca618798",
   "metadata": {},
   "source": [
    "The data is now prepared, and you are ready to create a bagging classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cfdf01",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Bagging\n",
    "\n",
    "In this section, you will build your first ensemble model by using the bootstrap aggregating, or bagging, approach. With this approach, you randomly draw multiple data subsets from the training set (with replacement) and train one model for each subset.\n",
    "\n",
    "The first approach will use multiple trees in the bagging model.\n",
    "\n",
    "### Data processing with a pipeline and a bagging ColumnTransformer\n",
    "\n",
    "You need to use different pipelines to handle the numerical, categorical, and text features. Then, you will combine them into a composite pipeline along with an estimator. To do this, you will use a [BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4efaee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### COLUMN_TRANSFORMER ###\n",
    "##########################\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"num_scaler\",\n",
    "            MinMaxScaler(),\n",
    "        )  # Shown in case it is needed. Not a must with decision trees.\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocess the categorical features\n",
    "# handle_unknown tells it to ignore (rather than throw an error for) any value\n",
    "# that was not present in the initial training set.\n",
    "categorical_processor = Pipeline(\n",
    "    [(\"cat_encoder\", OneHotEncoder(handle_unknown=\"ignore\"))]\n",
    ")\n",
    "\n",
    "# Preprocess the text feature\n",
    "text_processor_0 = Pipeline(\n",
    "    [(\"text_vect_0\", CountVectorizer(binary=True, max_features=150))]\n",
    ")\n",
    "\n",
    "# Combine all data preprocessors from above (add more if you choose to define more)\n",
    "# For each processor/step, specify a name, the actual process, and the features to be processed\n",
    "data_preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"numerical_pre\", numerical_processor, numerical_features),\n",
    "        (\"categorical_pre\", categorical_processor, categorical_features),\n",
    "        (\"text_pre_0\", text_processor_0, text_features[0]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "### PIPELINE ###\n",
    "################\n",
    "\n",
    "# Pipeline with all desired data transformers, along with an estimator\n",
    "# Later, you can set/reach the parameters by using the names issued - for hyperparameter tuning, for example\n",
    "\n",
    "#####################################################\n",
    "### Notice the pipeline using a BaggingClassifier ###\n",
    "#####################################################\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"data_preprocessing\", data_preprocessor),\n",
    "        (\n",
    "            \"bg\",\n",
    "            BaggingClassifier(\n",
    "                DecisionTreeClassifier(max_depth=25),  # Each tree has max_depth=25\n",
    "                n_estimators=10,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")  # Use 10 trees\n",
    "\n",
    "# Visualize the pipeline\n",
    "# This will be helpful especially when building more complex pipelines,\n",
    "# stringing together multiple preprocessing steps\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e31ccb",
   "metadata": {},
   "source": [
    "Now you can fit the bagging model, and see the training and test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2530919a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get training data to train the pipeline\n",
    "X_train = train_data[model_features]\n",
    "y_train = train_data[model_target]\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Use the fitted pipeline to make predictions on the training dataset\n",
    "train_predictions = pipeline.predict(X_train)\n",
    "print(confusion_matrix(y_train, train_predictions))\n",
    "print(classification_report(y_train, train_predictions))\n",
    "print(\"Accuracy (training):\", accuracy_score(y_train, train_predictions))\n",
    "\n",
    "# Get testing data to test the pipeline\n",
    "X_test = test_data[model_features]\n",
    "y_test = test_data[model_target]\n",
    "\n",
    "# Use the fitted pipeline to make predictions on the testing dataset\n",
    "test_predictions = pipeline.predict(X_test)\n",
    "print(confusion_matrix(y_test, test_predictions))\n",
    "print(classification_report(y_test, test_predictions))\n",
    "print(\"Accuracy (test):\", accuracy_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d055f82c",
   "metadata": {},
   "source": [
    "Using a bagging classifier isn't difficult because it only requires updating one line of code.\n",
    "\n",
    "Next, you will create a random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e74123",
   "metadata": {},
   "source": [
    "---\n",
    "## Random forest\n",
    "\n",
    "Now, you will try the second ensemble model: random forest. Random forest involves a similar ensemble process:\n",
    "- Draw random subsets (with replacement) from the original dataset.\n",
    "- Train individual trees with each subset.\n",
    "\n",
    "However, a difference is that random forest uses a randomly selected feature subset for each tree. As a rule of thumb, pick the `sqrt(# features)` as the number of random features for each tree and don't use any other features.\n",
    "\n",
    "\n",
    "The model is called in a similar way to the bagging method. You will replace the BaggingClassifier with a [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf780cd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### COLUMN_TRANSFORMER ###\n",
    "##########################\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"num_scaler\",\n",
    "            MinMaxScaler(),\n",
    "        )  # Shown in case it is needed. Not a must with decision trees.\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocess the categorical features\n",
    "# handle_unknown tells it to ignore (rather than throw an error for) any value\n",
    "# that was not present in the initial training set.\n",
    "categorical_processor = Pipeline(\n",
    "    [(\"cat_encoder\", OneHotEncoder(handle_unknown=\"ignore\"))]\n",
    ")\n",
    "\n",
    "# Preprocess the text feature\n",
    "text_processor_0 = Pipeline(\n",
    "    [(\"text_vect_0\", CountVectorizer(binary=True, max_features=150))]\n",
    ")\n",
    "\n",
    "# Combine all data preprocessors (add more if you choose to define more)\n",
    "# For each processor/step, specify a name, the actual process, and the features to be processed\n",
    "data_preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"numerical_pre\", numerical_processor, numerical_features),\n",
    "        (\"categorical_pre\", categorical_processor, categorical_features),\n",
    "        (\"text_pre_0\", text_processor_0, text_features[0]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "### PIPELINE ###\n",
    "################\n",
    "\n",
    "# Pipeline with all desired data transformers, along with an estimator\n",
    "# Later, you can set/reach the parameters by using the names issued - for hyperparameter tuning, for example\n",
    "\n",
    "##########################################################\n",
    "### Notice the pipeline using a RandomForestClassifier ###\n",
    "##########################################################\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"data_preprocessing\", data_preprocessor),\n",
    "        (\n",
    "            \"rf\",\n",
    "            RandomForestClassifier(\n",
    "                max_depth=25, n_estimators=100  # Each tree has max_depth=25\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")  # Use 100 trees\n",
    "\n",
    "# Visualize the pipeline\n",
    "# This will be helpful especially when building more complex pipelines,\n",
    "# stringing together multiple preprocessing steps\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9decbf5",
   "metadata": {},
   "source": [
    "Now you can fit the random forest model, and see the training and test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ec7be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get training data to train the pipeline\n",
    "X_train = train_data[model_features]\n",
    "y_train = train_data[model_target]\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Use the fitted pipeline to make predictions on the training dataset\n",
    "train_predictions = pipeline.predict(X_train)\n",
    "print(confusion_matrix(y_train, train_predictions))\n",
    "print(classification_report(y_train, train_predictions))\n",
    "print(\"Accuracy (training):\", accuracy_score(y_train, train_predictions))\n",
    "\n",
    "# Get testing data to test the pipeline\n",
    "X_test = test_data[model_features]\n",
    "y_test = test_data[model_target]\n",
    "\n",
    "# Use the fitted pipeline to make predictions on the testing dataset\n",
    "test_predictions = pipeline.predict(X_test)\n",
    "print(confusion_matrix(y_test, test_predictions))\n",
    "print(classification_report(y_test, test_predictions))\n",
    "print(\"Accuracy (test):\", accuracy_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd91aa1d",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h3><i>Try it yourself!</i></h3>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"images/challenge.png\" alt=\"Challenge\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">You can perform hyperparameter tuning on a random forest model.</p><br>\n",
    "    <p style=\" text-align: center; margin: auto;\">In the following code cell, run a grid search with the random forest classifier using <code>param_grid={'rf__max_depth': [25, 30, 45]}</code>.</p><br>\n",
    "    <p style=\" text-align: center; margin: auto;\">What is the best hyperparameter value after this run?</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e489f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write your code for grid search with param_grid={'rf__max_depth': [25, 30, 45]}\n",
    "\n",
    "# Parameter grid for GridSearch\n",
    "\n",
    "############### CODE HERE ###############\n",
    "\n",
    "param_grid={'rf__max_depth': [25, 30, 45]}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, # Base model\n",
    "                           param_grid, # Parameters to try\n",
    "                           cv = 5, # Apply five-fold cross validation\n",
    "                           verbose = 1, # Print summary\n",
    "                           n_jobs = -1 # Use all available processors\n",
    "                          )\n",
    "\n",
    "# Fit the GridSearch to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "############## END OF CODE ##############\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "# Get the best model out of GridSearchCV\n",
    "classifier = grid_search.best_estimator_\n",
    "\n",
    "# Fit the best model to the training data\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdceaa95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get testing data to test the classifier\n",
    "X_test = test_data[model_features]\n",
    "y_test = test_data[model_target]\n",
    "\n",
    "# Use the fitted model to make predictions on the test dataset\n",
    "# Testing data going through the pipeline is first imputed\n",
    "# (with means from the training set), scaled (with the min/max from the training data),\n",
    "# and finally used to make predictions.\n",
    "test_predictions = classifier.predict(X_test)\n",
    "\n",
    "print(\"Model performance on the test set:\")\n",
    "print(confusion_matrix(y_test, test_predictions))\n",
    "print(classification_report(y_test, test_predictions))\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba308da",
   "metadata": {},
   "source": [
    "---\n",
    "## Boosting\n",
    "\n",
    "The last ensemble model that you will try is boosting. This method builds multiple weak models sequentially. Each subsequent model attempts to boost performance overall by overcoming or reducing the errors of the previous model.\n",
    "\n",
    "You will use sklearn's [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871cd199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### COLUMN_TRANSFORMER ###\n",
    "##########################\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"num_scaler\",\n",
    "            MinMaxScaler(),\n",
    "        )  # Shown in case it is needed. Not a must with decision trees.\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocess the categorical features\n",
    "# handle_unknown tells it to ignore (rather than throw an error for) any value\n",
    "# that was not present in the initial training set.\n",
    "categorical_processor = Pipeline(\n",
    "    [(\"cat_encoder\", OneHotEncoder(handle_unknown=\"ignore\"))]\n",
    ")\n",
    "\n",
    "# Preprocess the text feature\n",
    "text_processor_0 = Pipeline(\n",
    "    [(\"text_vect_0\", CountVectorizer(binary=True, max_features=150))]\n",
    ")\n",
    "\n",
    "# Combine all data preprocessors (add more if you choose to define more)\n",
    "# For each processor/step, specify a name, the actual process, and the features to be processed\n",
    "data_preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"numerical_pre\", numerical_processor, numerical_features),\n",
    "        (\"categorical_pre\", categorical_processor, categorical_features),\n",
    "        (\"text_pre_0\", text_processor_0, text_features[0]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "### PIPELINE ###\n",
    "################\n",
    "\n",
    "# Pipeline with all desired data transformers, along with an estimator\n",
    "# Later, you can set/reach the parameters by using the names issued - for hyperparameter tuning, for example\n",
    "\n",
    "##############################################################\n",
    "### Notice the pipeline using a GradientBoostingClassifier ###\n",
    "##############################################################\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"data_preprocessing\", data_preprocessor),\n",
    "        (\n",
    "            \"gbc\",\n",
    "            GradientBoostingClassifier(\n",
    "                max_depth=10, n_estimators=100  # Each tree has max_depth=10\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")  # Use 100 trees\n",
    "\n",
    "# Visualize the pipeline\n",
    "# This will be helpful especially when building more complex pipelines,\n",
    "# stringing together multiple preprocessing steps\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0107a449",
   "metadata": {},
   "source": [
    "Now fit the model, and see the training and testing scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfa4049",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get training data to train the pipeline\n",
    "X_train = train_data[model_features]\n",
    "y_train = train_data[model_target]\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Use the fitted pipeline to make predictions on the training dataset\n",
    "train_predictions = pipeline.predict(X_train)\n",
    "print(confusion_matrix(y_train, train_predictions))\n",
    "print(classification_report(y_train, train_predictions))\n",
    "print(\"Accuracy (training):\", accuracy_score(y_train, train_predictions))\n",
    "\n",
    "# Get testing data to test the pipeline\n",
    "X_test = test_data[model_features]\n",
    "y_test = test_data[model_target]\n",
    "\n",
    "# Use the fitted pipeline to make predictions on the testing dataset\n",
    "test_predictions = pipeline.predict(X_test)\n",
    "print(confusion_matrix(y_test, test_predictions))\n",
    "print(classification_report(y_test, test_predictions))\n",
    "print(\"Accuracy (test):\", accuracy_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecc5ded",
   "metadata": {},
   "source": [
    "----\n",
    "## Conclusion\n",
    "\n",
    "This notebook provided an introduction to using Bagging, RandomForest, and GradientBoosting classifiers on the same dataset.\n",
    "\n",
    "## Next lab\n",
    "\n",
    "In the next lab, you will be introduced to fairness and bias mitigation in ML by exploring different types of bias that are present in data and practicing how to build various documentation sheets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
